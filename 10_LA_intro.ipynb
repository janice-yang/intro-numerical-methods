{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>\n",
    "\n",
    "Note:  This material largely follows the text \"Numerical Linear Algebra\" by Trefethen and Bau (SIAM, 1997) and is meant as a guide and supplement to the material presented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 3\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Linear Algebra\n",
    "\n",
    "Numerical methods for linear algebra problems lies at the heart of many numerical approaches and is something we will spend some time on.  Roughly we can break down problems that we would like to solve into two general problems, solving a system of equations\n",
    "\n",
    "$$A \\mathbf{x} = \\mathbf{b}$$\n",
    "\n",
    "and solving the eigenvalue problem\n",
    "\n",
    "$$A \\mathbf{v} = \\lambda \\mathbf{v}.$$\n",
    "\n",
    "We examine each of these problems separately and will evaluate some of the fundamental properties and methods for solving these problems. We will be careful in deciding how to evaluate the results of our calculations and try to gain some understanding of when and how they fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Problem Specification\n",
    "\n",
    "The number and power of the different tools made available from the study of linear algebra makes it an invaluable field of study. Before we dive in to numerical approximations we first consider some of the pivotal problems that numerical methods for linear algebra are used to address.\n",
    "\n",
    "For this discussion we will be using the common notation $m \\times n$ to denote the dimensions of a matrix $A$.  The $m$ refers to the number of rows and $n$ the number of columns.  If a matrix is square, i.e. $m = n$, then we will use the notation that $A$ is $m \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Systems of Equations\n",
    "\n",
    "The first type of problem is to find the solution to a linear system of equations.  If we have $m$ equations for $m$ unknowns it can be written in matrix/vector form,\n",
    "\n",
    "$$A \\mathbf{x} = \\mathbf{b}.$$\n",
    "\n",
    "For this example $A$ is an $m \\times m$ matrix, denoted as being in $\\mathbb{R}^{m\\times m}$, and $\\mathbf{x}$ and $\\mathbf{b}$ are column vectors with $m$ entries, denoted as $\\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: row vectors denoted as $\\mathbf{x}^\\top$ or $\\mathbf{b}^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Vandermonde Matrix\n",
    "\n",
    "We have data $(x_i, y_i), ~~ i = 1, 2, \\ldots, m$ that we want to fit a polynomial of order $m-1$.  Solving the linear system $A \\mathbf{p} = \\mathbf{y}$ does this for us where\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    1 & x_1 & x_1^2 & \\cdots & x_1^{m-1} \\\\\n",
    "    1 & x_2 & x_2^2 & \\cdots & x_2^{m-1} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    1 & x_m & x_m^2 & \\cdots & x_m^{m-1}\n",
    "\\end{bmatrix} \\quad \\quad \\mathbf{y} = \\begin{bmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and $\\mathbf{p}$ are the coefficients of the interpolating polynomial $\\mathcal{P}_N(x) = p_0 + p_1 x + p_2 x^2 + \\cdots + p_m x^{m-1}$. The solution to this system satisfies $\\mathcal{P}_N(x_i)=y_i$ for $i=1, 2, \\ldots, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Linear least squares 1\n",
    "\n",
    "In a similar case as above, say we want to fit a particular function (could be a polynomial) to a given number of data points except in this case we have more data points than free parameters.  In the case of polynomials this could be the same as saying we have $m$ data points but only want to fit a $n - 1$ order polynomial through the data where $n - 1 \\leq m$.  One of the common approaches to this problem is to minimize the \"least-squares\" error between the data and the resulting function:\n",
    "$$\n",
    "    E = \\left( \\sum^m_{i=1} |y_i - f(x_i)|^2 \\right )^{1/2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But how do we do this if our matrix $A$ is now $m \\times n$ and looks like\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "        1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n",
    "        1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "        1 & x_m & x_m^2 & \\cdots & x_m^{n-1}\n",
    "    \\end{bmatrix}?\n",
    "$$\n",
    "\n",
    "Turns out if we solve the system\n",
    "\n",
    "$$A^T A \\mathbf{x} = A^T \\mathbf{b}$$\n",
    "\n",
    "we can guarantee that the error is minimized in the least-squares sense[<sup>1</sup>](#footnoteRegression). (Although we will also show that this is **not the most numerically stable way to solve this problem**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: cannot solve if $A$ is singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Linear least squares 2\n",
    "\n",
    "Fitting a line through data that has random noise added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   , -1.   ],\n",
       "       [ 1.   , -0.895],\n",
       "       [ 1.   , -0.789],\n",
       "       [ 1.   , -0.684],\n",
       "       [ 1.   , -0.579],\n",
       "       [ 1.   , -0.474],\n",
       "       [ 1.   , -0.368],\n",
       "       [ 1.   , -0.263],\n",
       "       [ 1.   , -0.158],\n",
       "       [ 1.   , -0.053],\n",
       "       [ 1.   ,  0.053],\n",
       "       [ 1.   ,  0.158],\n",
       "       [ 1.   ,  0.263],\n",
       "       [ 1.   ,  0.368],\n",
       "       [ 1.   ,  0.474],\n",
       "       [ 1.   ,  0.579],\n",
       "       [ 1.   ,  0.684],\n",
       "       [ 1.   ,  0.789],\n",
       "       [ 1.   ,  0.895],\n",
       "       [ 1.   ,  1.   ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Least Squares Problem\n",
    "\n",
    "# First define the independent and dependent variables.\n",
    "N = 20\n",
    "x = numpy.linspace(-1.0, 1.0, N)\n",
    "y = x + numpy.random.random((N)) \n",
    "\n",
    "# Define the Vandermonde matrix based on our x-values\n",
    "A = numpy.array([ numpy.ones(x.shape), x]).T\n",
    "A = numpy.array([ numpy.ones(x.shape), x, x**2]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in slope = 0.04498595693536345, y-intercept = 0.09841273931733518\n",
      "[0.402 0.955]\n"
     ]
    }
   ],
   "source": [
    "# Determine the coefficients of the polynomial that will\n",
    "# result in the smallest sum of the squares of the residual.\n",
    "p = numpy.linalg.solve(numpy.dot(A.T, A), numpy.dot(A.T, y))\n",
    "print(\"Error in slope = %s, y-intercept = %s\" % (numpy.abs(p[1] - 1.0), numpy.abs(p[0] - 0.5)))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot it out, cuz pictures are fun!\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "f = numpy.zeros(x.shape)\n",
    "for i in range(len(p)):\n",
    "    f += p[i] * x**i\n",
    "axes.plot(x, y, 'ko')\n",
    "axes.plot(x, f, 'r')\n",
    "axes.set_title(\"Least Squares Fit to Data\")\n",
    "axes.set_xlabel(\"$x$\")\n",
    "axes.set_ylabel(\"$f(x)$ and $y_i$\")\n",
    "axes.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eigenproblems\n",
    "\n",
    "Eigenproblems come up in a variety of contexts and often are integral to many problem of scientific and engineering interest. It is such a powerful idea that it is not uncommon for us to take a problem and convert it into an eigenproblem. Here we introduce the idea and give some examples.\n",
    "\n",
    "As a review, if $A \\in \\mathbb{C}^{m\\times m}$ (a square matrix with complex values), a non-zero vector $\\mathbf{v}\\in\\mathbb{C}^m$ is an **eigenvector** of $A$ with a corresponding **eigenvalue** $\\lambda \\in \\mathbb{C}$ if \n",
    "\n",
    "$$A \\mathbf{v} = \\lambda \\mathbf{v}.$$\n",
    "\n",
    "One way to interpret the eigenproblem is that we are attempting to ascertain the \"action\" of the matrix $A$ on some subspace of $\\mathbb{C}^m$ where this action acts like scalar multiplication.  This subspace is called an **eigenspace**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: eigenproblems are nonlinear problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General idea of EigenProblems\n",
    "\n",
    "Rewriting the standard Eigen problem $A\\mathbf{v}=\\lambda\\mathbf{v}$ for $A \\in \\mathbb{C}^{m\\times m}$, $\\mathbf{v}\\in\\mathbb{C}^m$ as\n",
    "\n",
    "$$\n",
    "    (A - \\lambda I)\\mathbf{v} = 0\n",
    "$$ \n",
    "\n",
    "it becomes clear that for $\\mathbf{v}$ to be non-trivial (i.e. $\\neq \\mathbf{0}$), requires that the matrix $(A-\\lambda I)$ be singular,  \n",
    "\n",
    "This is equivalent to finding all values of $\\lambda$ such that $|A-\\lambda I| = 0$ (the determinant of singular matrices is always zero).  However, it can also be shown that \n",
    "\n",
    "$$\n",
    "   | A-\\lambda I| = P_m(\\lambda)\n",
    "$$\n",
    "\n",
    "which is a $m$th order polynomial in $\\lambda$.  Thus $P_m(\\lambda)=0$ implies the eigenvalues are the $m$ roots of $P$, and the **eigenspace** corresponding to $\\lambda_i$ is just $N(A-\\lambda_i I)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if $A$ is invertible (NOT singular), $\\mathbf{v}$ MUST be $0$.\n",
    "\n",
    "Singular matrix: determinant is $0$\n",
    "\n",
    "Determinant: alternating sum of products of all components from every row and column of $A$\n",
    "\n",
    "Can't find root of $P_m(\\lambda)$ if $m > 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving EigenProblems\n",
    "\n",
    "The temptation (and what) we usually teach in introductory linear algebra is to simply find the roots of $P_m(\\lambda)$.  However that would be **wrong**.  The best algorithms for finding Eigenvalues are completely unrelated to rootfinding as we shall see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "Compute the eigenspace of the matrix\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "        1 & 2 \\\\\n",
    "        2 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Recall that we can find the eigenvalues of a matrix by computing $\\det(A - \\lambda I) = 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this case we have\n",
    "$$\\begin{aligned}\n",
    "    A - \\lambda I &= \\begin{bmatrix}\n",
    "        1 & 2 \\\\\n",
    "        2 & 1\n",
    "    \\end{bmatrix} - \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\lambda\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        1 - \\lambda & 2 \\\\\n",
    "        2 & 1 - \\lambda\n",
    "    \\end{bmatrix}.\n",
    "\\end{aligned}$$\n",
    "The determinant of the matrix is\n",
    "$$\\begin{aligned}\n",
    "\\begin{vmatrix}\n",
    "    1 - \\lambda & 2 \\\\\n",
    "    2 & 1 - \\lambda\n",
    "\\end{vmatrix} &= (1 - \\lambda) (1 - \\lambda) - 2 \\cdot 2 \\\\\n",
    "&= 1 - 2 \\lambda + \\lambda^2 - 4 \\\\\n",
    "&= \\lambda^2 - 2 \\lambda - 3.\n",
    "\\end{aligned}$$\n",
    "This result is sometimes referred to as the characteristic equation of the matrix, $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Setting the determinant equal to zero we can find the eigenvalues as\n",
    "$$\\begin{aligned}\n",
    "    & \\\\\n",
    "    \\lambda &= \\frac{2 \\pm \\sqrt{4 - 4 \\cdot 1 \\cdot (-3)}}{2} \\\\\n",
    "    &= 1 \\pm 2 \\\\\n",
    "    &= -1 \\mathrm{~and~} 3\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The eigenvalues are used to determine the eigenvectors. The eigenvectors are found by going back to the equation $(A - \\lambda I) \\mathbf{v}_i = 0$ and solving for each vector.  A trick that works some of the time is to normalize each vector such that the first entry is 1 ($\\mathbf{v}_1 = 1$):\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 - \\lambda & 2 \\\\\n",
    "        2 & 1 - \\lambda\n",
    "    \\end{bmatrix} \\begin{bmatrix} 1 \\\\ v_2 \\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    1 - \\lambda + 2 v_2 &= 0 \\\\\n",
    "    v_2 &= \\frac{\\lambda - 1}{2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can check this by\n",
    "$$\\begin{aligned}\n",
    "    2 + \\left(1- \\lambda \\frac{\\lambda - 1}{2}\\right) & = 0\\\\\n",
    "    (\\lambda - 1)^2 - 4 &=0\n",
    "\\end{aligned}$$\n",
    "\n",
    "which by design is satisfied by our eigenvalues.  Another sometimes easier approach is to plug-in the eigenvalues to   find the Null space of $A-\\lambda I$ where the eigenvectors will be a basis for $N(A-\\lambda I)$.  The eigenvectors are therefore\n",
    "\n",
    "$$\\mathbf{v} = \\begin{bmatrix}1 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix}1 \\\\ 1 \\end{bmatrix}.$$\n",
    "\n",
    "Note that these are linearly independent (and because $A^T = A$, also orthogonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix-Vector Multiplication\n",
    "\n",
    "One of the most basic operations we can perform with matrices is to multiply them be a vector.  This matrix-vector product $A \\mathbf{x} = \\mathbf{b}$ is defined as\n",
    "$$\n",
    "    b_i = \\sum^n_{j=1} a_{ij} x_j \\quad \\text{where}\\quad i = 1, \\ldots, m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### row picture\n",
    "In addition to index form, we can consider matrix-vector as a sequence of inner products (dot-products between the rows of $A$ and the vector $\\mathbf{x}$.  \n",
    "\\begin{align}\n",
    "    \\mathbf{b} &= A \\mathbf{x}, \\\\\n",
    "     &= \n",
    "    \\begin{bmatrix}  \\mathbf{a}_1^T \\mathbf{x} \\\\ \\mathbf{a}_2^T \\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{a}_m^T \\mathbf{x}\\end{bmatrix}\n",
    "\\end{align}\n",
    "where $\\mathbf{a}_i^T$ is the $i$th **row** of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Operation Counts\n",
    "This view is convenient for calculating the **Operation counts** required for to compute $A\\mathbf{x}$.  If $A\\in\\mathbb{C}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{C}^n$.  Then just counting the number of multiplications involved to compute $A\\mathbf{x}$ is $O(??)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For $A\\in\\mathbb{C}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{C}^n$, $A\\mathbf{x}$ is $O(mn)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Column picture\n",
    "\n",
    "An alternative (and entirely equivalent way) to write the matrix-vector product is as a linear combination of the columns of $A$ where each column's weighting is $x_j$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{b} &= A \\mathbf{x}, \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}  &  &  &  \\\\  &  &  &  \\\\ \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\\\  &  &  &   \\\\  &  &  &  \\end{bmatrix}\n",
    "    \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\\\\n",
    "  &= x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This view will be useful later when we are trying to interpret various types of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One important property of the matrix-vector product is that is a **linear** operation, also known as a **linear operator**.  This means that the for any $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{C}^n$ and any $c \\in \\mathbb{C}$ we know that\n",
    "\n",
    "1. $A (\\mathbf{x} + \\mathbf{y}) = A\\mathbf{x} + A\\mathbf{y}$\n",
    "1. $A\\cdot (c\\mathbf{x}) = c A \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Vandermonde Matrix\n",
    "\n",
    "In the case where we have $m$ data points and want $m - 1$ order polynomial interpolant the matrix $A$ is a square, $m \\times m$, matrix as before.  Using the above interpretation the polynomial coefficients $p$ are the weights for each of the monomials that give exactly the $y$ values of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Numerical matrix-vector multiply\n",
    "\n",
    "Write a matrix-vector multiply function and check it with the appropriate `numpy` routine.  Also verify the linearity of the matrix-vector multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#A x = b\n",
    "#(m x n) (n x 1) = (m x 1)\n",
    "def matrix_vector_product(A, x):\n",
    "    # Should check that x and A have correct shape, but didn't\n",
    "    m, n = A.shape\n",
    "    b = numpy.zeros(m)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            b[i] += A[i, j] * x[j]\n",
    "    return b\n",
    "\n",
    "m = 4\n",
    "n = 3\n",
    "A = numpy.random.uniform(size=(m,n))\n",
    "x = numpy.random.uniform(size=(n))\n",
    "y = numpy.random.uniform(size=(n))\n",
    "c = numpy.random.uniform()\n",
    "b = matrix_vector_product(A, x)\n",
    "print(numpy.allclose(b, numpy.dot(A, x)))\n",
    "print(numpy.allclose(matrix_vector_product(A, (x + y)), matrix_vector_product(A, x) + matrix_vector_product(A, y)))\n",
    "print(numpy.allclose(matrix_vector_product(A, c * x), c*matrix_vector_product(A, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `numpy.dot` uses compiled C (more efficient than `for` loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "\n",
    "The matrix product with another matrix $A B = C$ is defined as\n",
    "$$\n",
    "    c_{ij} = \\sum^m_{k=1} a_{ik} b_{kj} = \\mathbf{a}_i^T\\mathbf{b}_j\n",
    "$$\n",
    "\n",
    "i.e. each component of $C$ is a dot-product between the $i$th row of $A$ and the $j$th column of $B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As with matrix-vector multiplication, Matrix-matrix multiplication can be thought of multiple ways\n",
    "\n",
    "* $m\\times p$ dot products\n",
    "* $A$ multiplying the columns of $B$\n",
    "$$\n",
    "    C = AB = \\begin{bmatrix} \n",
    "                A\\mathbf{b}_1 & A\\mathbf{b}_2 & \\ldots & A\\mathbf{b}_p\\\\ \n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "* Linear combinations of the rows of $B$\n",
    "$$\n",
    "C = AB = \\begin{bmatrix} \n",
    "                \\mathbf{a}_1^T B \\\\ \\mathbf{a}_2^T B \\\\ \\vdots \\\\ \\mathbf{a}_m^T B\\\\ \n",
    "             \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: $O(mnp)$, for $A\\in\\mathbb{C}^{m\\times n}$ and $B\\in\\mathbb{C}^{n\\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Questions\n",
    "* What are the dimensions of $A$ and $B$ so that the multiplication works?\n",
    "* What are the Operations Counts for Matrix-Matrix Multiplication?\n",
    "* Comment on the product $\\mathbf{c}=(AB)\\mathbf{x}$ vs. $\\mathbf{d} = A(B\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. num columns of $A$ = num rows of $B$\n",
    "2. $O(mnp)$\n",
    "3. $\\mathbf{c}=(AB)\\mathbf{x}$ is $O(m^3 + m^2)$, $\\mathbf{d} = A(B\\mathbf{x})$ is $O(m^2 + m^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Outer Product\n",
    "\n",
    "The product of two vectors $\\mathbf{u} \\in \\mathbb{C}^m$ and $\\mathbf{v} \\in \\mathbb{C}^n$ is a $m \\times n$ matrix where the columns are the vector $u$ multiplied by the corresponding value of $v$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{u} \\mathbf{v}^T &= \n",
    "    \\begin{bmatrix}  u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n  \\end{bmatrix}\n",
    "    \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}, \\\\\n",
    "    & = \\begin{bmatrix} v_1u_1 & \\cdots & v_n u_1 \\\\ \\vdots &  & \\vdots \\\\ v_1 u_m & \\cdots & v_n u_m \\end{bmatrix}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is useful to think of these as operations on the column vectors, and an equivalent way to express this relationship is \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{u} \\mathbf{v}^T &=\n",
    "    \\begin{bmatrix}  \\\\ \\mathbf{u} \\\\ \\\\  \\end{bmatrix}\n",
    "    \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}, \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}  &  &  &  \\\\  &  &  &  \\\\ \\mathbf{u}v_1  & \\mathbf{u} v_2  & \\cdots & \\mathbf{u} v_n  \\\\  &  &  & \\\\  &  &  &  \\end{bmatrix}, \\\\\n",
    "    & = \\begin{bmatrix} v_1u_1 & \\cdots & v_n u_1 \\\\ \\vdots &  & \\vdots \\\\ v_1 u_m & \\cdots & v_n u_m \\end{bmatrix}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### rank 1 updates\n",
    "\n",
    "We call any matrix of the form $\\mathbf{u}\\mathbf{v}^T$ a \"rank one matrix\"  ( because its rank r=?).  These sort of matrix operations are very common in numerical algorithms for orthogonalization, eigenvalues and the original page-rank algorithm for google.  Again, the order of operations is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Comment on the difference in values and operation counts between\n",
    "\n",
    "$$\n",
    "    \\mathbf{y} = (\\mathbf{u}\\mathbf{v}^T)\\mathbf{x} \\rightarrow O(mn)\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "    \\mathbf{y}' = \\mathbf{u}(\\mathbf{v}^T\\mathbf{x}) \\rightarrow O(m)\n",
    "$$\n",
    "for $\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{x}$, $\\mathbf{y}$, $\\mathbf{y}'\\in\\mathbb{R}^n$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example:  Upper Triangular Multiplication\n",
    "\n",
    "Consider the multiplication of a matrix $A \\in \\mathbb{C}^{m\\times n}$ and the **upper-triangular** matrix $R$ defined as the $n \\times n$ matrix with entries $r_{ij} = 1$ for $i \\leq j$ and $r_{ij} = 0$ for $i > j$.  The product can be written as\n",
    "$$\n",
    "    \\begin{bmatrix}  \\\\  \\\\ \\mathbf{b}_1 & \\cdots & \\mathbf{b}_n \\\\ \\\\ \\\\ \\end{bmatrix} = \\begin{bmatrix} \\\\ \\\\  \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\ \\\\ \\\\ \\end{bmatrix} \\begin{bmatrix} 1 & \\cdots & 1 \\\\  & \\ddots & \\vdots \\\\  &  & 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The columns of $B$ are then\n",
    "$$\n",
    "    \\mathbf{b}_j = A \\mathbf{r}_j = \\sum^j_{k=1} \\mathbf{a}_k\n",
    "$$\n",
    "so that $\\mathbf{b}_j$ is the sum of the first $j$ columns of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Write Matrix-Matrix Multiplication\n",
    "\n",
    "Write a function that computes matrix-matrix multiplication and demonstrate the following properties:\n",
    "1. $A (B + C) = AB + AC$ (for square matrices))\n",
    "1. $A (cB) = c AB$ where $c \\in \\mathbb{C}$\n",
    "1. $AB \\neq BA$ in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_matrix_product(A, B):\n",
    "    C = numpy.zeros((A.shape[0], B.shape[1]))\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            for k in range(A.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    return C\n",
    "\n",
    "\n",
    "m = 4\n",
    "n = 4\n",
    "p = 4\n",
    "A = numpy.random.uniform(size=(m, n))\n",
    "B = numpy.random.uniform(size=(n, p))\n",
    "C = numpy.random.uniform(size=(m, p))\n",
    "c = numpy.random.uniform()\n",
    "print(numpy.allclose(matrix_matrix_product(A, B), numpy.dot(A, B)))\n",
    "print(numpy.allclose(matrix_matrix_product(A, (B + C)), matrix_matrix_product(A, B) + matrix_matrix_product(A, C)))\n",
    "print(numpy.allclose(matrix_matrix_product(A, c * B), c*matrix_matrix_product(A, B)))\n",
    "print(numpy.allclose(matrix_matrix_product(A, B), matrix_matrix_product(B, A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### NumPy Products\n",
    "\n",
    "NumPy and SciPy contain routines that are optimized to perform matrix-vector and matrix-matrix multiplication.  Given two `ndarray`s you can take their product by using the `dot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[1.907 1.8   1.582 1.772 1.914]\n",
      "[[3.014 2.612 2.828 2.972 1.681]\n",
      " [2.774 1.881 2.742 2.187 1.574]\n",
      " [2.654 3.025 2.082 3.056 1.701]\n",
      " [3.282 2.617 3.254 2.953 2.1  ]\n",
      " [3.447 2.691 2.882 2.927 1.708]]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "m = 5\n",
    "# Matrix vector with identity\n",
    "A = numpy.identity(n)\n",
    "x = numpy.random.random(n)\n",
    "print(numpy.allclose(x, numpy.dot(A, x)))\n",
    "print(x-A.dot(x))\n",
    "print(A*x)\n",
    "\n",
    "# Matrix vector product\n",
    "A = numpy.random.random((m, n))\n",
    "print(numpy.dot(A, x))\n",
    "\n",
    "# Matrix matrix product\n",
    "B = numpy.random.random((n, m))\n",
    "print(numpy.dot(A, B))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Range and Null-Space\n",
    "\n",
    "#### Range\n",
    "- The **range** of a matrix $A \\in \\mathbb R^{m \\times n}$ (similar to any function), denoted as $\\text{range}(A)$, is the set of vectors that can be expressed as $A x$ for $x \\in \\mathbb R^n$.  \n",
    "- We can also then say that that $\\text{range}(A)$ is the space **spanned** by the columns of $A$.  In other words the columns of $A$ provide a basis for $\\text{range}(A)$, also called the **column space** of the matrix $A$.  \n",
    "- $C(A)$ controls the **existence** of solutions to $A\\mathbf{x}=\\mathbf{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Null-Space\n",
    "- Similarly the **null-space** of a matrix $A$, denoted $\\text{null}(A)$ is the set of vectors $x$ that satisfy $A x = 0$.\n",
    "- $N(A)$ controls the **uniqueness** of solutions to $A\\mathbf{x}=\\mathbf{b}$\n",
    "- A similar concept is the **rank** of the matrix $A$, denoted as $\\text{rank}(A)$, is the dimension of the column space.  A matrix $A$ is said to have **full-rank** if $\\text{rank}(A) = \\min(m, n)$.  This property also implies that the matrix mapping is **one-to-one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- Range determines whether $A\\mathbf{x} = \\mathbf{b}$ has a solution ($\\mathbf{b}$ must be in $C(A)$). \n",
    "- Nullspace controls uniqueness of solutions to $A\\mathbf{x} = \\mathbf{b}$.\n",
    "- Other 2 fundamental spaces: column space and null space of $A^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse\n",
    "\n",
    "A **non-singular** or **invertible** matrix is characterized as a matrix with full-rank.  This is related to why we know that the matrix is one-to-one, we can use it to transform a vector $x$ and using the inverse, denoted $A^{-1}$, we can map it back to the original matrix.  The familiar definition of this is\n",
    "\\begin{align*}\n",
    "    A \\mathbf{x} &= \\mathbf{b}, \\\\\n",
    "    A^{-1} A \\mathbf{x} & = A^{-1} \\mathbf{b}, \\\\\n",
    "    x &=A^{-1} \\mathbf{b}.\n",
    "\\end{align*}\n",
    "Since $A$ has full rank, its columns form a basis for $\\mathbb{R}^m$ and the vector $\\mathbf{b}$ must be in the column space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a number of important **properties of a non-singular matrix A**.  Here we list them as the following **equivalent statements**\n",
    "1. $A$ has an inverse $A^{-1}$\n",
    "1. $\\text{rank}(A) = m$\n",
    "1. $\\text{range}(A) = \\mathbb{C}^m$\n",
    "1. $\\text{null}(A) = \\mathbf{0}$\n",
    "1. 0 is not an eigenvalue of $A$\n",
    "1. $\\text{det}(A) \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Properties of invertible matrices\n",
    "\n",
    "Show that given an invertible matrix that the rest of the properties hold.  Make sure to search the `numpy` packages for relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.000e+00  5.778e-16 -1.843e-16]\n",
      " [ 9.104e-16  1.000e+00  9.109e-16]\n",
      " [ 6.793e-16 -4.149e-16  1.000e+00]]\n",
      "3\n",
      "N(A)= [ 0. -0. -0.]\n",
      "Eigenvalues = [1.854+0.j   0.174+0.03j 0.174-0.03j]\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "# Loop through random matrices until hit 100 or reach invertible matrix\n",
    "for n in range(100):\n",
    "    A = numpy.random.uniform(size=(m, m))\n",
    "    if numpy.linalg.det(A) != 0:\n",
    "        break\n",
    "        \n",
    "print(numpy.dot(numpy.linalg.inv(A), A))\n",
    "print(numpy.linalg.matrix_rank(A))\n",
    "print(\"N(A)= {}\".format(numpy.linalg.solve(A, numpy.zeros(m))))\n",
    "print(\"Eigenvalues = {}\".format(numpy.linalg.eigvals(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Orthogonal Vectors and Matrices\n",
    "\n",
    "Orthogonality is a very important concept in linear algebra that forms the basis of many of the modern methods used in numerical computations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two vectors are said to be *orthogonal* if their **inner-product** or **dot-product** defined as\n",
    "$$\n",
    "    < \\mathbf{x}, \\mathbf{y} > \\equiv (\\mathbf{x}, \\mathbf{y}) \\equiv \\mathbf{x}^T\\mathbf{y} \\equiv \\mathbf{x} \\cdot \\mathbf{y} = \\sum^m_{i=1} x_i y_i = 0\n",
    "$$\n",
    "Here we have shown the various notations you may run into (the inner-product is in-fact a general term for a similar operation for mathematical objects such as functions).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $\\langle \\mathbf{x},\\mathbf{y} \\rangle = 0$ then we say $\\mathbf{x}$ and $\\mathbf{y}$ are orthogonal.  The reason we use this terminology is that the inner-product of two vectors can also be written in terms of the angle between them where\n",
    "$$\n",
    "    \\cos \\theta = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{||\\mathbf{x}||_2~||\\mathbf{y}||_2}\n",
    "$$\n",
    "and $||\\mathbf{x}||_2$ is the Euclidean ($\\ell^2$) norm of the vector $\\mathbf{x}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write this in terms of the inner-product as well as\n",
    "$$\n",
    "    ||\\mathbf{x}||_2^2 = \\langle \\mathbf{x}, \\mathbf{x} \\rangle = \\mathbf{x}^T\\mathbf{x} = \\sum^m_{i=1} |x_i|^2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    ||\\mathbf{x}||_2 = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The generalization of the inner-product to complex spaces is defined as\n",
    "$$\n",
    "    \\langle x, y \\rangle = \\sum^m_{i=1} x_i^* y_i\n",
    "$$\n",
    "where $x_i^*$ is the complex-conjugate of the value $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Orthonormality\n",
    "\n",
    "Taking this idea one step further we can say a set of vectors $\\mathbf{x} \\in X$ are orthogonal to $\\mathbf{y} \\in Y$ if $\\forall \\mathbf{x},\\mathbf{y}$ $< \\mathbf{x}, \\mathbf{y} > = 0$.  If $\\forall \\mathbf{x},\\mathbf{y}$ $||\\mathbf{x}|| = 1$ and $||\\mathbf{y}|| = 1$ then they are also called orthonormal. Note that we dropped the 2 as a subscript to the notation for the norm of a vector. Later we will explore other ways to define a norm of a vector other than the Euclidean norm defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another concept that is related to orthogonality is linear-independence.  A set of vectors $\\mathbf{x} \\in X$ are **linearly independent** if $\\forall \\mathbf{x} \\in X$ that each $\\mathbf{x}$ cannot be written as a linear combination of the other vectors in the set $X$. \n",
    "\n",
    "\n",
    "\n",
    "An equivalent statement is that given a set of $n$ vectors $\\mathbf{x}_i$,  the only  set of scalars $c_i$ that satisfies\n",
    "$$\n",
    "    \\sum_{i=1}^n c_i\\mathbf{x}_i = \\mathbf{0}\n",
    "$$\n",
    "is if $c_i=0$ for all $i\\in[1,n]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can be related directly through the idea of projection.  If we have a set of vectors $\\mathbf{x} \\in X$ we can project another vector $\\mathbf{v}$ onto the vectors in $X$ by using the inner-product.  This is especially powerful if we have a set of **orthogonal** vectors $X$, which are said to **span** a space (or provide a **basis** for a space), s.t. any vector in the space spanned by $X$ can be expressed as a linear combination of the basis vectors $X$\n",
    "$$\n",
    "    \\mathbf{v} = \\sum^n_{i=1} \\, \\langle \\mathbf{v}, \\mathbf{x}_i \\rangle \\, \\mathbf{x}_i.\n",
    "$$\n",
    "Note if $\\mathbf{v} \\in X$ that \n",
    "$$\n",
    "    \\langle \\mathbf{v}, \\mathbf{x}_i \\rangle = 0 \\quad \\forall \\mathbf{x}_i \\in X \\setminus \\mathbf{v}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looping back to matrices, the column space of a matrix is spanned by its linearly independent columns.  Any vector $v$ in the column space can therefore be expressed via the equation above.  A special class of matrices are called **unitary** matrices when complex-valued and **orthogonal** when purely real-valued if the columns of the matrix are orthonormal to each other.  Importantly this implies that for a unitary matrix $Q$ we know the following\n",
    "\n",
    "1. $Q^* = Q^{-1}$\n",
    "1. $Q^*Q = I$\n",
    "\n",
    "where $Q^*$ is called the **adjoint** of $Q$.  The adjoint is defined as the transpose of the original matrix with the entries being the complex conjugate of each entry as the notation implies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example if we have the matrix\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    Q &= \\begin{bmatrix} q_{11} & q_{12} \\\\ q_{21} & q_{22} \\\\ q_{31} & q_{32} \\end{bmatrix} \\quad \\text{then} \\\\\n",
    "    Q^* &= \\begin{bmatrix} q^*_{11} & q^*_{21} & q^*_{31} \\\\ q^*_{12} & q^*_{22} & q^*_{32} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The important part of being an unitary matrix is that the projection onto the column space of the matrix $Q$ preserves geometry in an Euclidean sense, i.e. preserves the Cartesian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Norms\n",
    "\n",
    "Norms (and also measures) provide a means for measure the \"size\" or distance in a space.  In general a norm is a function, denoted by $||\\cdot||$, that maps $\\mathbb{C}^m \\rightarrow \\mathbb{R}$.  In other words we stick in a multi-valued object and get a single, real-valued number out the other end.  All norms satisfy the properties:\n",
    "\n",
    "1. $~~~~||\\mathbf{x}|| \\geq 0$\n",
    "1. $~~~~||\\mathbf{x}|| = 0$ only if $\\mathbf{x} = \\mathbf{0}$\n",
    "1. $$||\\mathbf{x} + \\mathbf{y}||\\leq ||\\mathbf{x}|| + ||\\mathbf{y}||$$ **(triangle inequality)**\n",
    "1. $~~~||c \\mathbf{x}|| = |c| ~ ||\\mathbf{x}||$ where $c \\in \\mathbb{C}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a number of relevant norms that we can define beyond the Euclidean norm, also know as the 2-norm or $\\ell_2$ norm:\n",
    "\n",
    "1. $\\ell_1$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_1 = \\sum^m_{i=1} |x_i|,\n",
    "$$\n",
    "1. $\\ell_2$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_2 = \\left( \\sum^m_{i=1} |x_i|^2 \\right)^{1/2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. $\\ell_p$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_p = \\left( \\sum^m_{i=1} |x_i|^p \\right)^{1/p}, \\quad \\quad 1 \\leq p < \\infty,\n",
    "$$\n",
    "1. $\\ell_\\infty$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_\\infty = \\max_{1\\leq i \\leq m} |x_i|,\n",
    "$$\n",
    "1. weighted $\\ell_p$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_{W_p} = \\left( \\sum^m_{i=1} |w_i x_i|^p \\right)^{1/p}, \\quad \\quad 1 \\leq p < \\infty,\n",
    "$$\n",
    "\n",
    "These are also related to other norms denoted by capital letters ($L_2$ for instance).  In this case we use the lower-case notation to denote finite or discrete versions of the infinite dimensional counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Comparisons Between Norms\n",
    "\n",
    "Compute the norms given some vector $\\mathbf{x}$ and compare their values.  Verify the properties of the norm for one of the norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def pnorm(x, p):\n",
    "    \"\"\" return the vector p norm of a vector\n",
    "    parameters:\n",
    "    -----------\n",
    "    x: numpy array\n",
    "        vector\n",
    "    p: float or numpy.inf\n",
    "        value of p norm such that ||x||_p = (sum(|x_i|^p))^{1/p} for p< inf\n",
    "        for infinity norm return max(abs(x))\n",
    "    returns:\n",
    "    --------\n",
    "    pnorm: float\n",
    "        pnorm of x\n",
    "    \"\"\"\n",
    "    if p == numpy.inf:\n",
    "        norm = numpy.max(numpy.abs(x))\n",
    "    else:\n",
    "        norm = numpy.sum(x**p)**(1./p)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m = 10\n",
    "p = 4\n",
    "x = numpy.random.uniform(size=m)        \n",
    "ell_1 = pnorm(x, 1)\n",
    "ell_2 = pnorm(x, 2)\n",
    "ell_p = pnorm(x, p)\n",
    "ell_infty = pnorm(x, numpy.inf)\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print()\n",
    "print(\"L_1   = {}\\nL_2   = {}\\nL_{}   = {}\\nL_inf = {}\".format(ell_1, ell_2, p, ell_p, ell_infty))\n",
    "\n",
    "y = numpy.random.uniform(size=m)\n",
    "print()\n",
    "print(\"Properties of norms:\")\n",
    "\n",
    "print('y = {}\\n'.format(y))\n",
    "p = 2\n",
    "print('||x+y||_{p}         = {nxy}\\n||x||_{p} + ||y||_{p} = {nxny}'.format(\n",
    "    p=p,nxy=pnorm(x+y, p), nxny=pnorm(x, p) + pnorm(y, p)))\n",
    "c = 0.1\n",
    "print('||c x||_{} = {}'.format(p,pnorm(c * x, p)))\n",
    "print(' c||x||_{} = {}'.format(p,c * pnorm(x, p)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Norms\n",
    "\n",
    "The most direct way to consider a matrix norm is those induced by a vector-norm.  Given a vector norm, we can define a matrix norm as the smallest number $C$ that satisfies the inequality\n",
    "$$\n",
    "    ||A \\mathbf{x}||_{m} \\leq C ||\\mathbf{x}||_{n}.\n",
    "$$\n",
    "or as the supremum of the ratios so that\n",
    "$$\n",
    "    C = \\sup_{\\mathbf{x}\\in\\mathbb{C}^n ~ \\mathbf{x}\\neq\\mathbf{0}} \\frac{||A \\mathbf{x}||_{m}}{||\\mathbf{x}||_n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Noting that $||A \\mathbf{x}||$ lives in the column space and $||\\mathbf{x}||$ on the domain we can think of the matrix norm as the \"size\" of the matrix that maps the domain to the range.  Also noting that if $||\\mathbf{x}||_n = 1$ we also satisfy the condition we can write the induced matrix norm as\n",
    "$$\n",
    "    ||A||_{(m,n)} = \\sup_{\\mathbf{x} \\in \\mathbb{C}^n ~ ||\\mathbf{x}||_{n} = 1} ||A \\mathbf{x}||_{m}.\n",
    "$$\n",
    "\n",
    "This definition has a **geometric interpretation**.  The set of all $\\mathbf{x}$ such that $||\\mathbf{x}||_n = 1$ is the \"unit sphere\" in $\\mathbb{C}^n$.  So the induced matrix norm is the largest vector in the deformed \"sphere\" and measures how much the matrix distorts the unit sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Induced Matrix Norms\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "    A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "Compute the induced-matrix norm of $A$ for the vector norms $\\ell_2$ and $\\ell_\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\ell^2$: For both of the requested norms the unit-length vectors $[1, 0]$ and $[0, 1]$ can be used to give an idea of what the norm might be and provide a lower bound.  \n",
    "\n",
    "$$\n",
    "    ||A||_2 = \\sup_{x \\in \\mathbb{R}^n} \\left( ||A \\cdot [1, 0]^T||_2, ||A \\cdot [0, 1]^T||_2 \\right )\n",
    "$$\n",
    "\n",
    "computing each of the norms we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} &= \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "which translates into the norms $||A \\cdot [1, 0]^T||_2 = 1$ and $||A \\cdot [0, 1]^T||_2 = 2 \\sqrt{2}$.  This implies that the $\\ell_2$ induced matrix norm of $A$ is at least $||A||_{2} = 2 \\sqrt{2} \\approx 2.828427125$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The exact value of $||A||_2$ can be computed using the spectral radius defined as\n",
    "$$\n",
    "    \\rho(A) = \\max_{i} |\\lambda_i|,\n",
    "$$\n",
    "where $\\lambda_i$ are the eigenvalues of $A$.  With this we can compute the $\\ell_2$ norm of $A$ as\n",
    "$$\n",
    "    ||A||_2 = \\sqrt{\\rho(A^\\ast A)}\n",
    "$$\n",
    "\n",
    "Computing the norm again here we find\n",
    "$$\n",
    "    A^\\ast A = \\begin{bmatrix} 1 & 0 \\\\ 2 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 8 \\end{bmatrix}\n",
    "$$\n",
    "which has eigenvalues \n",
    "$$\n",
    "    \\lambda = \\frac{1}{2}\\left(9 \\pm \\sqrt{65}\\right )\n",
    "$$\n",
    "so $||A||_2 \\approx 2.9208096$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The actual  induced 2-norm of a matrix can be derived using the Singular Value Decomposition (SVD) and is simply the largest singular value $\\sigma_1$.\n",
    "\n",
    "**Proof**: \n",
    "Given that every Matrix $A\\in\\mathbb{C}^{m\\times n}$ can be factored into its SVD (see notebook 10.1):\n",
    "\n",
    "$$\n",
    "    A = U\\Sigma V^*\n",
    "$$\n",
    "\n",
    "where $U\\in\\mathbb{C}^{m\\times m}$ and $V\\in\\mathbb{C}^{n\\times n}$ are unitary matrices with the property $U^*U=I$ and $V^*V=I$ (of their respective sizes) and $\\Sigma$ is a real diagonal matrix of singular values $\\sigma_1 \\geq\\sigma_2\\geq...\\sigma_n\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the 2-norm squared of a  square matrix is \n",
    "$$\n",
    "    ||A||^2_2 = \\sup_{\\mathbf{x} \\in \\mathbb{C}^n ~ ||\\mathbf{x}||_2 = 1} ||A \\mathbf{x}||_2^2 = \\mathbf{x}^TA^*A\\mathbf{x}\n",
    "$$\n",
    "but $A^*A = V\\Sigma^2V^*$ so\n",
    "\n",
    "\\begin{align}\n",
    "    ||A \\mathbf{x}||_2^2 &= \\mathbf{x}^*V\\Sigma^2V^*\\mathbf{x} \\\\\n",
    "                         &= \\mathbf{y}^*\\Sigma^2\\mathbf{y} \\quad\\mathrm{where}\\quad \\mathbf{y}=V^*\\mathbf{x}\\\\\n",
    "                         &= \\sum_{i=1}^n \\sigma_i^2|y_i|^2\\\\\n",
    "                         &\\leq \\sigma_1^2\\sum_{i=1}^n |y_i|^2 = \\sigma_i^2||\\mathbf{y}||_2\\\\\n",
    "\\end{align}  \n",
    "\n",
    "but if $||\\mathbf{x}||_2 = 1$ (i.e. is a unit vector), then so is $\\mathbf{y}$ because unitary matrices don't change the length of vectors.  So it follows that \n",
    "$$\n",
    "    ||A||_2 = \\sigma_1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = numpy.array([[1, 2], [0, 2]])\n",
    "\n",
    "#calculate the SVD(A)\n",
    "U, S, Vt = numpy.linalg.svd(A)\n",
    "\n",
    "print('Singular_values = {}'.format(S))\n",
    "print('||A||_2 = {}'.format(S.max()))\n",
    "print('||A||_2 = {}'.format(numpy.linalg.norm(A, ord=2)))\n",
    "\n",
    "# more fun facts about the SVD\n",
    "#print(U.T.dot(U))\n",
    "#print(Vt.T.dot(Vt))\n",
    "#print(A - numpy.dot(U,numpy.dot(numpy.diag(S),Vt)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  Other useful norms of a  Matrix\n",
    "\n",
    "The 2-norm of a matrix can be expensive to compute,  however there are other norms that are equivalent that can be directly computed from  the components of $A$.  For example\n",
    "\n",
    "* The induced 1-norm is simply max of the 1-norm of the **columns** of $A$\n",
    "\n",
    "$$\n",
    "||A \\mathbf{x}||_1 = || \\sum^n_{j=1} x_j \\mathbf{a}_j ||_1 \\leq \\sum^n_{j=1} |x_j| ||\\mathbf{a}_j||_1 \\leq \\max_{1\\leq j\\leq n} ||\\mathbf{a}_j||_1 ||\\mathbf{x}||_1 =  \\max_{1\\leq j\\leq n} ||\\mathbf{a}_j||_1\n",
    "$$\n",
    "\n",
    "* The induce $\\infty$-norm is simply the max of the 1-norm of **rows** of $A$\n",
    "\n",
    "$$\n",
    "||A \\mathbf{x}||_\\infty = \\max_{1 \\leq i \\leq m} | \\mathbf{a}^*_i \\mathbf{x} | \\leq \\max_{1 \\leq i \\leq m} ||\\mathbf{a}^*_i||_1\n",
    "$$\n",
    "because the largest unit vector on the unit sphere in the $\\infty$ norm is a vector of 1's.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  Example:\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$$ ||A||_1 = 4, \\quad ||A||_\\infty = 3$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 1-norm of A\n",
    "normA_1 = numpy.max(numpy.sum(numpy.abs(A), axis=0))\n",
    "print('||A||_1 = {}'.format(normA_1))\n",
    "print('||A||_1 = {}'.format(numpy.linalg.norm(A, ord=1)))\n",
    "# calculate the infinity norm of A\n",
    "normA_inf = numpy.max(numpy.sum(numpy.abs(A), axis=1))\n",
    "print('||A||_inf = {}'.format(normA_inf))\n",
    "print('||A||_inf = {}'.format(numpy.linalg.norm(A, ord=numpy.inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of the most useful ways to think about matrix norms is as a transformation of a unit-ball to an ellipse.  Depending on the norm in question, the norm will be some combination of the resulting ellipse.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = numpy.array([[1, 2], [0, 2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ============\n",
    "# 2-norm\n",
    "\n",
    "# Unit-ball\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"2-Norm: $||A||_2 = ${:3.4f}\".format(numpy.linalg.norm(A,ord=2)),fontsize=16)\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 1, aspect='equal')\n",
    "axes.add_artist(plt.Circle((0.0, 0.0), 1.0, edgecolor='r', facecolor='none'))\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.set_title(\"Unit Ball\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "\n",
    "# Image\n",
    "# Compute some geometry\n",
    "u, s, v = numpy.linalg.svd(A)\n",
    "theta = numpy.empty(A.shape[0])\n",
    "ellipse_axes = numpy.empty(A.shape)\n",
    "theta[0] = numpy.arccos(u[0][0]) / numpy.linalg.norm(u[0], ord=2)\n",
    "theta[1] = theta[0] - numpy.pi / 2.0\n",
    "for i in range(theta.shape[0]):\n",
    "    ellipse_axes[0, i] = s[i] * numpy.cos(theta[i])\n",
    "    ellipse_axes[1, i] = s[i] * numpy.sin(theta[i])\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 2, aspect='equal')\n",
    "axes.add_artist(patches.Ellipse((0.0, 0.0), 2 * s[0], 2 * s[1], theta[0] * 180.0 / numpy.pi,\n",
    "                                edgecolor='r', facecolor='none'))\n",
    "for i in range(A.shape[0]):\n",
    "    axes.arrow(0.0, 0.0, ellipse_axes[0, i] - head_length * numpy.cos(theta[i]), \n",
    "                         ellipse_axes[1, i] - head_length * numpy.sin(theta[i]), \n",
    "                         head_width=head_width, color='k')\n",
    "draw_unit_vectors(axes, A, head_width=0.2)\n",
    "axes.set_title(\"Images Under A\")\n",
    "axes.set_xlim((-s[0] + 0.1, s[0] + 0.1))\n",
    "axes.set_ylim((-s[0] + 0.1, s[0] + 0.1))\n",
    "axes.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Note: that this code is a bit fragile to angles that go beyond pi\n",
    "# due to the use of arccos.\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def draw_unit_vectors(axes, A, head_width=0.1):\n",
    "    head_length = 1.5 * head_width\n",
    "    image_e = numpy.empty(A.shape)\n",
    "    angle = numpy.empty(A.shape[0])\n",
    "    image_e[:, 0] = numpy.dot(A, numpy.array((1.0, 0.0)))\n",
    "    image_e[:, 1] = numpy.dot(A, numpy.array((0.0, 1.0)))\n",
    "    for i in range(A.shape[0]):\n",
    "        angle[i] = numpy.arccos(image_e[0, i] / numpy.linalg.norm(image_e[:, i], ord=2))\n",
    "        axes.arrow(0.0, 0.0, image_e[0, i] - head_length * numpy.cos(angle[i]), \n",
    "                             image_e[1, i] - head_length * numpy.sin(angle[i]), \n",
    "                             head_width=head_width, color='b', alpha=0.5)\n",
    "    \n",
    "head_width = 0.2\n",
    "head_length = 1.5 * head_width\n",
    "# ============\n",
    "# 1-norm\n",
    "# Unit-ball\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "fig.suptitle(\"1-Norm: $||A||_1 = {}$\".format(numpy.linalg.norm(A,ord=1)), fontsize=16)\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 1, aspect='equal')\n",
    "axes.plot((1.0, 0.0, -1.0, 0.0, 1.0), (0.0, 1.0, 0.0, -1.0, 0.0), 'r')\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.set_title(\"Unit Ball\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "\n",
    "# Image\n",
    "axes = fig.add_subplot(1, 2, 2, aspect='equal')\n",
    "axes.plot((1.0, 2.0, -1.0, -2.0, 1.0), (0.0, 2.0, 0.0, -2.0, 0.0), 'r')\n",
    "draw_unit_vectors(axes, A, head_width=0.2)\n",
    "\n",
    "axes.set_title(\"Images Under A\")\n",
    "axes.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### $\\infty$-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ============\n",
    "# infty-norm\n",
    "# Unit-ball\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"$\\infty$-Norm: $||A||_\\infty = {}$\".format(numpy.linalg.norm(A,ord=numpy.inf)),fontsize=16)\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 1, aspect='equal')\n",
    "axes.plot((1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, -1.0, 1.0), 'r')\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.set_title(\"Unit Ball\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "\n",
    "# Image\n",
    "# Geometry - Corners are A * ((1, 1), (1, -1), (-1, 1), (-1, -1))\n",
    "# Symmetry implies we only need two.  Here we just plot two\n",
    "u = numpy.empty(A.shape)\n",
    "u[:, 0] = numpy.dot(A, numpy.array((1.0, 1.0)))\n",
    "u[:, 1] = numpy.dot(A, numpy.array((-1.0, 1.0)))\n",
    "theta[0] = numpy.arccos(u[0, 0] / numpy.linalg.norm(u[:, 0], ord=2))\n",
    "theta[1] = numpy.arccos(u[0, 1] / numpy.linalg.norm(u[:, 1], ord=2))\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 2, aspect='equal')\n",
    "axes.plot((3, 1, -3, -1, 3), (2, 2, -2, -2, 2), 'r')\n",
    "for i in range(A.shape[0]):\n",
    "    axes.arrow(0.0, 0.0, u[0, i] - head_length * numpy.cos(theta[i]), \n",
    "                         u[1, i] - head_length * numpy.sin(theta[i]), \n",
    "                         head_width=head_width, color='k')\n",
    "\n",
    "draw_unit_vectors(axes, A, head_width=0.2)\n",
    "axes.set_title(\"Images Under A\")\n",
    "axes.set_xlim((-4.1, 4.1))\n",
    "axes.set_ylim((-3.1, 3.1))\n",
    "axes.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cauchy-Schwarz and Hölder Inequalities\n",
    "\n",
    "Computing matrix norms where $p \\neq 1$ or $\\infty$ is more difficult unfortunately.  We have a couple of tools that can be useful however.  \n",
    "\n",
    " - **Cauchy-Schwarz Inequality**:  For the special case where $p=q=2$, for any vectors $\\mathbf{x}$ and $\\mathbf{y}$\n",
    "$$\n",
    "    |\\mathbf{x}^*\\mathbf{y}| \\leq ||\\mathbf{x}||_2 ||\\mathbf{y}||_2\n",
    "$$\n",
    " - **Hölder's Inequality**:  Turns out this holds in general if given a $p$ and $q$ that satisfy $1/p + 1/q = 1$ with $1 \\leq p, q \\leq \\infty$\n",
    "\n",
    "$$\n",
    "    |\\mathbf{x}^*\\mathbf{y}| \\leq ||\\mathbf{x}||_p ||\\mathbf{y}||_q.\n",
    "$$\n",
    "\n",
    "**Note**: this is essentially what we used in the proof of the $\\infty-$norm with $p=1$ and $q=\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### General Matrix Norms (induced and  non-induced)\n",
    "\n",
    "In general matrix-norms have the following properties whether they are induced from a vector-norm or not:\n",
    "1. $||A|| \\geq 0$ and $||A|| = 0$ only if $A = 0$\n",
    "1. $||A + B|| \\leq ||A|| + ||B||$ (Triangle Inequality)\n",
    "1. $||c A|| = |c| ||A||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most widely used matrix norm not induced by a vector norm is the **Frobenius norm** defined by\n",
    "$$\n",
    "    ||A||_F = \\left( \\sum^m_{i=1} \\sum^n_{j=1} |A_{ij}|^2 \\right)^{1/2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Invariance under unitary multiplication\n",
    "\n",
    "One important property of the matrix 2-norm (and Forbenius norm) is that multiplication by a unitary matrix does not change the product (kind of like multiplication by 1).  In general for any $A \\in \\mathbb{C}^{m\\times n}$ and unitary matrix $Q \\in \\mathbb{C}^{m \\times m}$ we have\n",
    "\\begin{align*}\n",
    "    ||Q A||_2 &= ||A||_2 \\\\ ||Q A||_F &= ||A||_F.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<sup>1</sup><span id=\"footnoteRegression\"> http://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf</span>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
